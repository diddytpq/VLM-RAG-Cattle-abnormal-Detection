{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc755e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from torchvision.io.video import read_video\n",
    "from transformers import AutoProcessor, VideoMAEModel\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# --- [1. 설정 및 VideoMAE 로드] ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# VideoMAE (Retriever)\n",
    "retriever_model_id = \"MCG-NJU/videomae-base\" \n",
    "feature_extractor = AutoProcessor.from_pretrained(retriever_model_id)\n",
    "retriever_model = VideoMAEModel.from_pretrained(retriever_model_id).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a626e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_embedding(video_path):\n",
    "    \"\"\"VideoMAE를 사용해 영상 특징 추출 (전처리 포함)\"\"\"\n",
    "    try:\n",
    "        rgb, audio, info = read_video(video_path, pts_unit='sec')\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {video_path}: {e}\")\n",
    "        return torch.zeros(1, 768).cpu()\n",
    "\n",
    "    # 프레임 샘플링 (16개)\n",
    "    if rgb.size(0) > 16:\n",
    "        indices = torch.linspace(0, rgb.size(0) - 1, 16).long()\n",
    "        video_frames = rgb[indices]\n",
    "    else:\n",
    "        indices = torch.linspace(0, rgb.size(0) - 1, 16).long()\n",
    "        video_frames = rgb[indices]\n",
    "\n",
    "    video_frames_numpy = list(video_frames.numpy())\n",
    "    inputs = feature_extractor(video_frames_numpy, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = retriever_model(**inputs.to(device))\n",
    "        video_emb = outputs.last_hidden_state.mean(dim=1) \n",
    "        \n",
    "    video_emb = video_emb / video_emb.norm(p=2, dim=-1, keepdim=True)\n",
    "    return video_emb.cpu()\n",
    "\n",
    "def rag_inference(new_video_path, database_matrix):\n",
    "    \"\"\"가장 유사한 영상 1개의 인덱스 반환\"\"\"\n",
    "    query_emb = get_video_embedding(new_video_path)\n",
    "    similarity = torch.mm(query_emb, database_matrix.t())\n",
    "    scores, indices = torch.topk(similarity, k=1)\n",
    "    return indices[0].item() # .item()으로 스칼라 값 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e78ca01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchvision/io/_video_deprecation_warning.py:9: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- [2. Database Indexing (VideoMAE)] ---\n",
    "print(\"Indexing Database...\")\n",
    "\n",
    "video_path = os.path.join(os.getcwd(), \"datasets\", \"videos\")\n",
    "video_path_list = os.listdir(video_path)\n",
    "# JSON 로드 (경로는 실제 환경에 맞게 수정 필요)\n",
    "text_json = json.load(open(\"./datasets/train_video_reason_annotation.json\"))\n",
    "\n",
    "# 빠른 검색을 위한 매핑\n",
    "video_to_conversations = {}\n",
    "for item in text_json:\n",
    "    json_video_name = os.path.basename(item[\"video\"])\n",
    "    video_to_conversations[json_video_name] = item[\"conversations\"]\n",
    "\n",
    "dataset_embeddings = []\n",
    "dataset_embeddings_video_name = []\n",
    "dataset_context_map = {} # 인덱스 -> 텍스트 매핑\n",
    "\n",
    "for i, video_name in enumerate(video_path_list):\n",
    "    full_path = os.path.join(video_path, video_name)\n",
    "    emb = get_video_embedding(full_path)\n",
    "    dataset_embeddings.append(emb)\n",
    "    dataset_embeddings_video_name.append(video_name)\n",
    "    \n",
    "    # 해당 영상의 'GPT 답변'만 추출하여 Context로 저장\n",
    "    if video_name in video_to_conversations:\n",
    "        convs = video_to_conversations[video_name]\n",
    "        # 보통 conversations 구조: [{'from': 'human', ...}, {'from': 'gpt', 'value': '정답 텍스트'}]\n",
    "        # GPT의 답변을 합쳐서 문맥으로 사용\n",
    "        context_text = \" \".join([turn['value'] for turn in convs if turn['from'] == 'gpt'])\n",
    "        dataset_context_map[i] = context_text\n",
    "    else:\n",
    "        dataset_context_map[i] = \"No description available.\"\n",
    "\n",
    "dataset_embeddings = torch.cat(dataset_embeddings)\n",
    "# 전체 정규화\n",
    "dataset_embeddings = dataset_embeddings / dataset_embeddings.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "# --- [3. vLLM (Qwen-VL) 설정] ---\n",
    "os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbacb9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-15 10:11:47 [utils.py:253] non-default args: {'trust_remote_code': True, 'max_model_len': 8192, 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'enforce_eager': True, 'model': '/root/backup/workspace/RAG-example/weights/Qwen3-VL-4B-Thinking-FP8'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-15 10:11:47 [model.py:637] Resolved architecture: Qwen3VLForConditionalGeneration\n",
      "INFO 12-15 10:11:47 [model.py:1750] Using max model len 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 10:11:48,070\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-15 10:11:48 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 12-15 10:11:48 [vllm.py:601] Enforce eager set, overriding optimization level to -O0\n",
      "INFO 12-15 10:11:48 [vllm.py:707] Cudagraph is disabled under eager mode\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3992004)\u001b[0;0m INFO 12-15 10:11:52 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='/root/backup/workspace/RAG-example/weights/Qwen3-VL-4B-Thinking-FP8', speculative_config=None, tokenizer='/root/backup/workspace/RAG-example/weights/Qwen3-VL-4B-Thinking-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=/root/backup/workspace/RAG-example/weights/Qwen3-VL-4B-Thinking-FP8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['+quant_fp8', 'all', '+quant_fp8'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3992004)\u001b[0;0m INFO 12-15 10:11:53 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.0.216:60981 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3992004)\u001b[0;0m INFO 12-15 10:11:53 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3992004)\u001b[0;0m INFO 12-15 10:11:55 [gpu_model_runner.py:3467] Starting to load model /root/backup/workspace/RAG-example/weights/Qwen3-VL-4B-Thinking-FP8...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3992004)\u001b[0;0m INFO 12-15 10:11:55 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3992004)\u001b[0;0m INFO 12-15 10:11:55 [default_loader.py:308] Loading weights took 0.13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  8.28it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  8.27it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3992004)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=3992004)\u001b[0;0m INFO 12-15 10:11:56 [gpu_model_runner.py:3549] Model loading took 5.2960 GiB memory and 0.296728 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3992004)\u001b[0;0m INFO 12-15 10:11:56 [gpu_model_runner.py:4306] Encoder cache will be initialized with a budget of 151250 tokens, and profiled with 1 video items of the maximum feature size.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3992004)\u001b[0;0m WARNING 12-15 10:12:01 [fp8_utils.py:777] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/utils/configs/N=6144,K=2560,device_name=NVIDIA_GeForce_RTX_4080,dtype=fp8_w8a8,block_shape=[128,128].json\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3992004)\u001b[0;0m WARNING 12-15 10:12:01 [fp8_utils.py:777] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/utils/configs/N=2560,K=4096,device_name=NVIDIA_GeForce_RTX_4080,dtype=fp8_w8a8,block_shape=[128,128].json\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3992004)\u001b[0;0m WARNING 12-15 10:12:01 [fp8_utils.py:777] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/utils/configs/N=19456,K=2560,device_name=NVIDIA_GeForce_RTX_4080,dtype=fp8_w8a8,block_shape=[128,128].json\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3992004)\u001b[0;0m WARNING 12-15 10:12:01 [fp8_utils.py:777] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/utils/configs/N=2560,K=9728,device_name=NVIDIA_GeForce_RTX_4080,dtype=fp8_w8a8,block_shape=[128,128].json\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3992004)\u001b[0;0m INFO 12-15 10:12:02 [gpu_worker.py:359] Available KV cache memory: 2.43 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3992004)\u001b[0;0m INFO 12-15 10:12:02 [kv_cache_utils.py:1286] GPU KV cache size: 17,696 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3992004)\u001b[0;0m INFO 12-15 10:12:02 [kv_cache_utils.py:1291] Maximum concurrency for 8,192 tokens per request: 2.16x\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3992004)\u001b[0;0m INFO 12-15 10:12:02 [core.py:254] init engine (profile, create kv cache, warmup model) took 6.26 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3992004)\u001b[0;0m WARNING 12-15 10:12:02 [vllm.py:608] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=3992004)\u001b[0;0m INFO 12-15 10:12:02 [vllm.py:707] Cudagraph is disabled under eager mode\n",
      "INFO 12-15 10:12:03 [llm.py:343] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "# vLLM 입력 준비 함수\n",
    "def prepare_inputs_for_vllm(messages, processor):\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # Qwen2-VL 특화 처리 (Dynamic Resolution 등)\n",
    "    image_inputs, video_inputs, video_kwargs = process_vision_info(\n",
    "        messages,\n",
    "        image_patch_size=processor.image_processor.patch_size,\n",
    "        return_video_kwargs=True,\n",
    "        return_video_metadata=True\n",
    "    )\n",
    "    \n",
    "    mm_data = {}\n",
    "    if image_inputs is not None:\n",
    "        mm_data['image'] = image_inputs\n",
    "    if video_inputs is not None:\n",
    "        mm_data['video'] = video_inputs\n",
    "\n",
    "    return {\n",
    "        'prompt': text,\n",
    "        'multi_modal_data': mm_data,\n",
    "        \"mm_processor_kwargs\": video_kwargs, # 중요: Qwen2-VL의 grid thw 정보 전달\n",
    "    }\n",
    "\n",
    "# 모델 로드 (경로 주의)\n",
    "checkpoint_path = os.path.join(os.getcwd(), \"weights\", \"Qwen3-VL-4B-Thinking-FP8\")\n",
    "# Qwen-VL용 Processor 로드\n",
    "processor = AutoProcessor.from_pretrained(checkpoint_path, trust_remote_code=True)\n",
    "\n",
    "llm = LLM(\n",
    "    model=checkpoint_path,\n",
    "    trust_remote_code=True,\n",
    "    gpu_memory_utilization=0.80, # 필요시 조절\n",
    "    max_model_len=8192,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=torch.cuda.device_count(),\n",
    "    seed=0,\n",
    "    # Qwen2-VL 계열은 limit_mm_per_prompt 등을 설정해야 할 수 있음 (최신 버전은 자동)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "701b0f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-15 10:17:26] INFO vision_process.py:320: decord:  video_path='/root/backup/workspace/RAG-example/datasets/val/mounting/crop203.mp4', total_frames=128, video_fps=15.118110236220472, time=0.024s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Error on crop203.mp4: EngineCore encountered an issue. See stack trace (above) for the root cause.\n",
      "Total FP Count: 0\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0.1, # 사실적 분석을 위해 낮게 설정\n",
    "    max_tokens=256,\n",
    "    stop=[\"<|endoftext|>\", \"<|im_end|>\"]\n",
    ")\n",
    "\n",
    "# --- [4. RAG + vLLM Inference Pipeline] ---\n",
    "print(\"Start Inference...\")\n",
    "new_video_path = os.path.join(os.getcwd(), \"datasets\", \"val\", \"mounting\")\n",
    "video_list = os.listdir(new_video_path)\n",
    "\n",
    "FP = 0 # False Positive Count\n",
    "\n",
    "# 배치 처리를 원하면 리스트에 모아서 llm.generate에 한 번에 넘길 수도 있음\n",
    "# 여기서는 로직 확인을 위해 Loop 방식 유지\n",
    "for video_name in video_list:\n",
    "    target_full_path = os.path.join(new_video_path, video_name)\n",
    "    \n",
    "    # [Step 1] RAG: 유사한 과거 영상 찾기\n",
    "    idx = rag_inference(target_full_path, dataset_embeddings)\n",
    "    \n",
    "    # [Step 2] Context 추출\n",
    "    retrieved_video_name = dataset_embeddings_video_name[idx]\n",
    "    retrieved_context = dataset_context_map[idx]\n",
    "    \n",
    "    # 디버깅: 검색된 영상 정보\n",
    "    # print(f\"Target: {video_name} -> Similar to: {retrieved_video_name}\")\n",
    "    \n",
    "    # [Step 3] Prompt 구성 (RAG 적용)\n",
    "    # 검색된 'retrieved_context'를 프롬프트에 주입하여 모델이 참고하게 함\n",
    "    # system_msg = \"You are an expert in analyzing livestock behavior from videos.\"\n",
    "    user_msg = (\n",
    "        f\"Context: I found a similar historical video which was described as follows: \\\"{retrieved_context}\\\"\\n\\n\"\n",
    "        \"Task: Based on the visual content of the provided video and the context above, \"\n",
    "        \"analyze whether the cow's behavior is Normal or Abnormal. \"\n",
    "        \"Provide a reasoning and conclude with 'Status: Normal' or 'Status: Abnormal'.\"\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"video\", \n",
    "                    \"video\": target_full_path,\n",
    "                    \"max_pixels\": 360 * 420, # 필요시 조절\n",
    "                    \"fps\": 1.0, \n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": user_msg},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # [Step 4] vLLM Inference\n",
    "    try:\n",
    "        inputs = prepare_inputs_for_vllm(messages, processor)\n",
    "        \n",
    "        # llm.generate는 요청 리스트를 받습니다.\n",
    "        outputs = llm.generate([inputs], sampling_params=sampling_params, use_tqdm=False)\n",
    "        generated_text = outputs[0].outputs[0].text\n",
    "        \n",
    "        print(f\"Video: {video_name}\")\n",
    "        print(f\"Ref Video: {retrieved_video_name}\")\n",
    "        print(f\"Output: {generated_text}\\n\" + \"-\"*30)\n",
    "        \n",
    "        # [Step 5] 결과 분석 (False Positive Check)\n",
    "        # \"Normal\"이라고 판단했는지 확인 (대소문자 무시)\n",
    "        if \"Normal\" in generated_text or \"status: normal\" in generated_text.lower():\n",
    "            # 만약 실제 정답이 Abnormal이어야 하는데 Normal이라고 했다면 FP (혹은 정의에 따라 다름)\n",
    "            # 여기서는 코드 작성자분의 의도(특정 단어 포함 여부 카운트)를 따름\n",
    "            FP += 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Inference Error on {video_name}: {e}\")\n",
    "\n",
    "    break\n",
    "\n",
    "print(f\"Total FP Count: {FP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ff90a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ddc085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
