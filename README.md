# ğŸ„ VLM + RAG ê¸°ë°˜ ì†Œ í–‰ë™ ë¶„ì„ ì‹œìŠ¤í…œ

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

### í”„ë¡œì íŠ¸ ëª…
**Vision Language Modelê³¼ Retrieval-Augmented Generationì„ í™œìš©í•œ ì†Œ ë§ˆìš´íŒ… í–‰ë™ ìë™ íƒì§€ ì‹œìŠ¤í…œ**

### í”„ë¡œì íŠ¸ ê¸°ê°„
2025ë…„ ~ 2026ë…„

### í”„ë¡œì íŠ¸ ëª©ì 
ì¶•ì‚°ì—…ì—ì„œ ì†Œì˜ ë°œì • í–‰ë™(ë§ˆìš´íŒ…)ì„ ìë™ìœ¼ë¡œ íƒì§€í•˜ì—¬ ë²ˆì‹ íš¨ìœ¨ì„±ì„ ë†’ì´ê³ , ë†ê°€ì˜ ê´€ë¦¬ ë¶€ë‹´ì„ ì¤„ì´ëŠ” AI ê¸°ë°˜ ì˜ìƒ ë¶„ì„ ì‹œìŠ¤í…œ ê°œë°œ

---

## ğŸ¯ í•µì‹¬ ê¸°ìˆ  ë° ì•„í‚¤í…ì²˜

### ì‹œìŠ¤í…œ êµ¬ì„±ë„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RAG-Enhanced VLM Pipeline                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   [ì…ë ¥ ë¹„ë””ì˜¤]                                                          â”‚
â”‚        â”‚                                                                â”‚
â”‚        â–¼                                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚   â”‚  VideoMAE   â”‚â”€â”€â”€â”€â–¶â”‚  Vector Database    â”‚                          â”‚
â”‚   â”‚  Retriever  â”‚     â”‚  (Embedding Index)  â”‚                          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚        â”‚                       â”‚                                        â”‚
â”‚        â”‚ Query Embedding       â”‚ Top-K Similar Videos                   â”‚
â”‚        â–¼                       â–¼                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚   â”‚        Similarity Search (Cosine)       â”‚                          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                       â”‚                                                 â”‚
â”‚                       â–¼ Retrieved Context                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚   â”‚      Qwen3-VL-4B-Thinking (vLLM)        â”‚                          â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚                          â”‚
â”‚   â”‚  â”‚ System: Visual Reasoning AI     â”‚    â”‚                          â”‚
â”‚   â”‚  â”‚ Context: Similar Video Desc.    â”‚    â”‚                          â”‚
â”‚   â”‚  â”‚ Video: Input Video Frames       â”‚    â”‚                          â”‚
â”‚   â”‚  â”‚ Task: Classify Normal/Mounting  â”‚    â”‚                          â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                       â”‚                                                 â”‚
â”‚                       â–¼                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚   â”‚    ì¶œë ¥: Status: Normal / Mounting      â”‚                          â”‚
â”‚   â”‚    + Chain-of-Thought Reasoning         â”‚                          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì‚¬ìš© ê¸°ìˆ  ìŠ¤íƒ

| ë¶„ë¥˜ | ê¸°ìˆ  | ì„¤ëª… |
|------|------|------|
| **VLM** | Qwen3-VL-4B-Thinking-FP8 | ë¹„ë””ì˜¤ ì´í•´ ë° ì¶”ë¡ ì´ ê°€ëŠ¥í•œ Vision-Language ëª¨ë¸ |
| **Retriever** | VideoMAE (MCG-NJU/videomae-base) | ë¹„ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ ë° ì„ë² ë”© ìƒì„± |
| **ì¶”ë¡  ì—”ì§„** | vLLM | ê³ ì† LLM ì¶”ë¡  ì„œë¹™ |
| **ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬** | PyTorch, Transformers | ëª¨ë¸ ë¡œë”© ë° ì²˜ë¦¬ |
| **ë²¡í„° ê²€ìƒ‰** | Cosine Similarity (Torch) | ìœ ì‚¬ë„ ê¸°ë°˜ ë¹„ë””ì˜¤ ê²€ìƒ‰ |

---

## ğŸ”¬ ê¸°ìˆ ì  êµ¬í˜„ ìƒì„¸

### 1. VideoMAE ê¸°ë°˜ ë¹„ë””ì˜¤ ì„ë² ë”©

```python
def get_video_embedding(video_path):
    """VideoMAEë¥¼ ì‚¬ìš©í•´ ì˜ìƒ íŠ¹ì§• ì¶”ì¶œ"""
    # ë¹„ë””ì˜¤ ë¡œë“œ ë° 16í”„ë ˆì„ ìƒ˜í”Œë§
    rgb, audio, info = read_video(video_path, pts_unit='sec')
    
    # ê· ë“± ê°„ê²©ìœ¼ë¡œ 16ê°œ í”„ë ˆì„ ì„ íƒ
    indices = torch.linspace(0, rgb.size(0) - 1, 16).long()
    video_frames = rgb[indices]
    
    # VideoMAEë¡œ íŠ¹ì§• ì¶”ì¶œ
    with torch.no_grad():
        outputs = retriever_model(**inputs.to(device))
        video_emb = outputs.last_hidden_state.mean(dim=1)
    
    # L2 ì •ê·œí™”
    video_emb = video_emb / video_emb.norm(p=2, dim=-1, keepdim=True)
    return video_emb
```

### 2. RAG ê¸°ë°˜ ìœ ì‚¬ ë¹„ë””ì˜¤ ê²€ìƒ‰

```python
def rag_inference(new_video_path, database_matrix):
    """ê°€ì¥ ìœ ì‚¬í•œ ì˜ìƒ ê²€ìƒ‰"""
    query_emb = get_video_embedding(new_video_path)
    similarity = torch.mm(query_emb, database_matrix.t())
    scores, indices = torch.topk(similarity, k=1)
    return indices[0].item()
```

### 3. Context-Augmented í”„ë¡¬í”„íŠ¸ êµ¬ì„±

```python
# RAG ì ìš© ì‹œ í”„ë¡¬í”„íŠ¸
user_msg = (
    f"Context: I found a similar historical video which was described as follows: "
    f"\"{retrieved_context}\"\n\n"
    "Task: Based on the visual content of the provided video and the context above, "
    "analyze whether the cow's behavior is Normal or mounting. "
    "Provide a reasoning and conclude with 'Status: Normal' or 'Status: Mounting'."
)
```

### 4. Chain-of-Thought ì¶”ë¡ 

ëª¨ë¸ì€ `<think>...</think>` íƒœê·¸ ë‚´ì—ì„œ ì¶”ë¡  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ì¶œë ¥í•˜ì—¬, íŒë‹¨ì˜ ê·¼ê±°ë¥¼ íˆ¬ëª…í•˜ê²Œ ì œê³µí•©ë‹ˆë‹¤.

---

## ğŸ“Š ë°ì´í„°ì…‹ êµ¬ì„±

### í•™ìŠµ ë°ì´í„°
| í•­ëª© | ìˆ˜ëŸ‰ | ì„¤ëª… |
|------|------|------|
| í•™ìŠµ ë¹„ë””ì˜¤ | 332ê°œ | ë§ˆìš´íŒ…/ì •ìƒ í–‰ë™ ì˜ìƒ |
| ì–´ë…¸í…Œì´ì…˜ | Human-GPT ëŒ€í™”ìŒ | ì§ˆë¬¸-ë‹µë³€ í˜•ì‹ì˜ í–‰ë™ ì„¤ëª… |

### ê²€ì¦ ë°ì´í„°
| í•­ëª© | ìˆ˜ëŸ‰ | ì„¤ëª… |
|------|------|------|
| ë§ˆìš´íŒ… ì˜ìƒ | 43ê°œ | ë°œì • í–‰ë™ì´ í¬í•¨ëœ ì˜ìƒ |
| ì •ìƒ ì˜ìƒ | 43ê°œ | ì¼ìƒ í–‰ë™ ì˜ìƒ |

### ì–´ë…¸í…Œì´ì…˜ í˜•ì‹ ì˜ˆì‹œ
```json
{
  "video": "./datasets/videos/crop001.mp4",
  "conversations": [
    {
      "from": "human",
      "value": "<video>\nBased on an analysis of the video focusing on the cows' behavior, 
                was there any mounting behavior?\nOptions:\n(A) YES\n(B) NO"
    },
    {
      "from": "gpt",
      "value": "Answer: YES\nReason: Three cows are interacting closely; one cow clearly 
                mounts another by positioning its front legs on the back of the other cow..."
    }
  ]
}
```

---

## ğŸ“ˆ ì‹¤í—˜ ê²°ê³¼ ë° ì„±ëŠ¥ ë¶„ì„

### ì‹¤í—˜ ì„¤ì •
- **ëª¨ë¸**: Qwen3-VL-4B-Thinking-FP8
- **ì¶”ë¡  ì„¤ì •**: temperature=0.2, max_tokens=5120, repetition_penalty=1.1
- **ë¹„ë””ì˜¤ ì²˜ë¦¬**: 3.0 FPS ìƒ˜í”Œë§

### ì„±ëŠ¥ ë¹„êµ (ë§ˆìš´íŒ… ì˜ìƒ 43ê°œ í…ŒìŠ¤íŠ¸)

| ë°©ë²• | Precision | íŠ¹ì§• |
|------|-----------|------|
| **VLM Only** | - | ì»¨í…ìŠ¤íŠ¸ ì—†ì´ ìˆœìˆ˜ ë¹„ì „ ë¶„ì„ |
| **VLM + RAG** | - | ìœ ì‚¬ ì˜ìƒì˜ ì„¤ëª…ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš© |

### RAG ì ìš© íš¨ê³¼ ë¶„ì„

#### âœ… RAG ì ìš©ì˜ ì¥ì 
1. **ì¼ê´€ëœ íŒë‹¨ ê¸°ì¤€**: ìœ ì‚¬ ì‚¬ë¡€ì˜ ì„¤ëª…ì„ ì°¸ì¡°í•˜ì—¬ íŒë‹¨ ê¸°ì¤€ í†µì¼
2. **ìƒì„¸í•œ ì¶”ë¡ **: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ë” êµ¬ì²´ì ì¸ í–‰ë™ ë¶„ì„
3. **ì˜¤íƒì§€ ê°ì†Œ**: ì •ìƒ ì˜ìƒì—ì„œì˜ False Positive ê°ì†Œ

#### ğŸ“ ê²°ê³¼ ì˜ˆì‹œ ë¹„êµ

**RAG ë¯¸ì ìš© ì‹œ (VLM Only):**
```
Video: crop203.mp4
Output: "One cow is clearly rearing on its hind legs... Status: Mounting"
```

**RAG ì ìš© ì‹œ:**
```
Video: crop203.mp4
Retrieved Video: Normal063.mp4
Output: "Comparing to the historical example which described a cow 'getting up'...
        the current video depicts TWO cows interacting with distinct postures...
        Status: Mounting"
```

---

## ğŸ”‘ í•µì‹¬ ì„±ê³¼ ë° ê¸°ì—¬

### 1. RAG ê¸°ë°˜ ë¹„ë””ì˜¤ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì„¤ê³„
- VideoMAEë¥¼ Retrieverë¡œ í™œìš©í•œ ë¹„ë””ì˜¤ ì„ë² ë”© ì‹œìŠ¤í…œ êµ¬ì¶•
- ìœ ì‚¬ ì˜ìƒ ê²€ìƒ‰ + VLM ì¶”ë¡ ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸

### 2. ë„ë©”ì¸ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
- ì¶•ì‚° ë„ë©”ì¸ì— ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ê³„
- Chain-of-Thought ì¶”ë¡ ì„ í†µí•œ íŒë‹¨ ê·¼ê±° íˆ¬ëª…í™”

### 3. ì‹¤ìš©ì  ì¶”ë¡  ì‹œìŠ¤í…œ êµ¬í˜„
- vLLM ê¸°ë°˜ ê³ ì† ì¶”ë¡  íŒŒì´í”„ë¼ì¸
- FP8 ì–‘ìí™” ëª¨ë¸ í™œìš©ìœ¼ë¡œ GPU ë©”ëª¨ë¦¬ íš¨ìœ¨í™”

---

## ğŸ›  í”„ë¡œì íŠ¸ êµ¬ì¡°

```
RAG-example/
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ train_video_reason_annotation.json  # í•™ìŠµ ë°ì´í„° ì–´ë…¸í…Œì´ì…˜
â”‚   â”œâ”€â”€ videos/                             # í•™ìŠµìš© ë¹„ë””ì˜¤ (332ê°œ)
â”‚   â””â”€â”€ val/
â”‚       â”œâ”€â”€ mounting/                       # ê²€ì¦ìš© ë§ˆìš´íŒ… ì˜ìƒ (43ê°œ)
â”‚       â””â”€â”€ normal/                         # ê²€ì¦ìš© ì •ìƒ ì˜ìƒ (43ê°œ)
â”œâ”€â”€ weights/
â”‚   â””â”€â”€ Qwen3-VL-4B-Thinking-FP8/          # VLM ëª¨ë¸ ê°€ì¤‘ì¹˜
â”œâ”€â”€ test_rag.py                            # RAG íŒŒì´í”„ë¼ì¸ ì¶”ë¡  ì½”ë“œ
â”œâ”€â”€ test.py                                # VLM Only ì¶”ë¡  ì½”ë“œ
â”œâ”€â”€ test_text_only.py                      # í…ìŠ¤íŠ¸ ì „ìš© í…ŒìŠ¤íŠ¸
â”œâ”€â”€ result.json                            # VLM Only ë§ˆìš´íŒ… ê²°ê³¼
â”œâ”€â”€ result_rag.json                        # RAG ì ìš© ë§ˆìš´íŒ… ê²°ê³¼
â”œâ”€â”€ result_normal.json                     # VLM Only ì •ìƒ ê²°ê³¼
â””â”€â”€ result_rag_normal.json                 # RAG ì ìš© ì •ìƒ ê²°ê³¼
```

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### í™˜ê²½ ì„¤ì •
```bash
pip install torch torchvision transformers vllm qwen-vl-utils
```

### RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
```bash
python test_rag.py
```

### VLM Only ì‹¤í–‰
```bash
python test.py
```

---

## ğŸ“Œ í–¥í›„ ê°œì„  ë°©í–¥

1. **Multi-Modal RAG í™•ì¥**: í…ìŠ¤íŠ¸ + ë¹„ë””ì˜¤ + ë©”íƒ€ë°ì´í„° í†µí•© ê²€ìƒ‰
2. **ì‹¤ì‹œê°„ ì¶”ë¡  ìµœì í™”**: ìŠ¤íŠ¸ë¦¬ë° ë¹„ë””ì˜¤ ì²˜ë¦¬ ì§€ì›
3. **Fine-tuning**: ë„ë©”ì¸ íŠ¹í™” ë°ì´í„°ë¡œ VLM ë¯¸ì„¸ì¡°ì •
4. **ì•™ìƒë¸” ê¸°ë²•**: ë‹¤ì¤‘ ëª¨ë¸ ê²°ê³¼ ê²°í•©ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
5. **ì›¹ ê¸°ë°˜ ë°ëª¨**: Gradio/Streamlit ê¸°ë°˜ ì‹œì—° ì¸í„°í˜ì´ìŠ¤ êµ¬ì¶•

---

## ğŸ“š ì°¸ê³  ë¬¸í—Œ

- Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond
- VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training
- Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks

---

## ğŸ‘¨â€ğŸ’» ê°œë°œì ì •ë³´

**ì—­í• **: AI/ML Engineer  
**ë‹´ë‹¹ ì—…ë¬´**: 
- ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„¤ê³„ ë° êµ¬í˜„
- RAG ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ê³„
- í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
- ëª¨ë¸ í‰ê°€ ë° ë¶„ì„

---

*ë³¸ í”„ë¡œì íŠ¸ëŠ” ì¶•ì‚° ë¶„ì•¼ AI ì ìš©ì„ í†µí•œ ìŠ¤ë§ˆíŠ¸íŒœ êµ¬í˜„ì˜ ì¼í™˜ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.*
